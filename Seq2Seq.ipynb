{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ger = spacy.load('de')\n",
    "spacy_eng = spacy.load('en')\n",
    "\n",
    "def tokenizer_ger(text):\n",
    "    return [token.text for token in spacy_ger.tokenizer(text)]\n",
    "\n",
    "def tokenizer_eng(text):\n",
    "    return[token.text for token in spacy_eng.tokenizer(text)]\n",
    "\n",
    "german = Field(tokenize=tokenizer_ger, lower=True, init_token='<sos>', eos_token='<eos>')\n",
    "english = Field(tokenize=tokenizer_eng, lower=True, init_token='<sos>', eos_token='<eos>')\n",
    "\n",
    "train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n",
    "                                                        fields=(german, english))\n",
    "\n",
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size,\n",
    "                num_layers, dropout_layer):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_layer)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_layer)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    #Note here that we are also passing in the hidden and cell state.\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # shape of x: (1, N, embedding_size)\n",
    "        #The '1' is because we are sending in a word at a time through the decoder.\n",
    "        #So, to add the '1', we can unsqueeze the array.\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, hidden_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        #shape of outputs: (1, N, hidden_size)\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        # shape predictions: (1, N, length_of_vocab)\n",
    "        #When sending it to the fully connected layer, we don't need the '1'\n",
    "        #therefore, we can squeeze it to get rid of the '1'.\n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        #Grab the start token.\n",
    "        x = target[0]\n",
    "        \n",
    "        #The teacher_force_ratio is applicable to the decoder.\n",
    "        #If you vizualize the decoder, the output here is a sequence of english words.\n",
    "        #The output from one node is going to be the input to the the next.\n",
    "        #However, if the output is the wrong word, this means the input to the next node is wrong as well.\n",
    "        #What we can do to prevent this is feed input from the target sentence into the decoder time to time.\n",
    "        #So 50% of the time, input will be words from the target sequence.\n",
    "        #If it is more, this would then not train the model properly as it is given all the answers.\n",
    "        \n",
    "        #In this for loop, we can see that the parameter to the decoder, 'x', is sometimes target\n",
    "        #and sometimes it is the the word from output.\n",
    "        #We start however with the first word in the target i.e. start token.\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            best_guess = output.argmax(1)\n",
    "            \n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training hyperparameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "#Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device('cpu')\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "#Tensorboard\n",
    "writer = SummaryWriter(f'runs/loss_plot')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "454it [8:40:35, 68.80s/it]  \n"
     ]
    }
   ],
   "source": [
    "#The sort_within_batch and sort_key TRIES to make sure that the batches contain words of equal\n",
    "#length so that it doesn't waste too much compute in padding.\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key= lambda x: len(x.src),\n",
    "    device=device)\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size,\n",
    "                     num_layers, enc_dropout).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, \n",
    "                     output_size, num_layers, dec_dropout).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = english.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch [{epoch} / {num_epochs}]')\n",
    "    \n",
    "    for batch_idx, batch in tqdm(enumerate(train_iterator)):\n",
    "        input_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "        \n",
    "        output = model(input_data, target)\n",
    "        #output shape: (target_len, batch_size, output_dim)\n",
    "        \n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar('Training loss', loss, global_step=step)\n",
    "        step += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
